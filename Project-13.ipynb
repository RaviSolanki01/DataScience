{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topics\n",
    "Review of model evaluation procedures\n",
    "Steps for K-fold cross-validation\n",
    "Comparing cross-validation to train/test split\n",
    "Cross-validation recommendations\n",
    "Cross-validation example: parameter tuning\n",
    "Cross-validation example: model selection\n",
    "Cross-validation example: feature selection\n",
    "Improvements to cross-validation\n",
    "Resources\n",
    "This tutorial is derived from Data School's Machine Learning with scikit-learn tutorial. I added my own notes so anyone, including myself, can refer to this tutorial without watching the videos.\n",
    "\n",
    "1. Review of model evaluation procedures\n",
    "Motivation: Need a way to choose between machine learning models\n",
    "\n",
    "Goal is to estimate likely performance of a model on out-of-sample data\n",
    "Initial idea: Train and test on the same data\n",
    "\n",
    "But, maximizing training accuracy rewards overly complex models which overfit the training data\n",
    "Alternative idea: Train/test split\n",
    "\n",
    "Split the dataset into two pieces, so that the model can be trained and tested on different data\n",
    "Testing accuracy is a better estimate than training accuracy of out-of-sample performance\n",
    "Problem with train/test split\n",
    "It provides a high variance estimate since changing which observations happen to be in the testing set can significantly change testing accuracy\n",
    "Testing accuracy can change a lot depending on a which observation happen to be in the testing set\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "# read in the iris data\n",
    "iris = load_iris()\n",
    "\n",
    "# create X (features) and y (response)\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "# use train/test split with different random_state values\n",
    "# we can change the random_state values that changes the accuracy scores\n",
    "# the accuracy changes a lot\n",
    "# this is why testing accuracy is a high-variance estimate\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=6)\n",
    "\n",
    "# check classification accuracy of KNN with K=5\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "metrics.accuracy_score(y_test, y_pred)\n",
    "0.97368421052631582\n",
    "Question: What if we created a bunch of train/test splits, calculated the testing accuracy for each, and averaged the results together?\n",
    "\n",
    "Answer: That's the essense of cross-validation!\n",
    "\n",
    "2. Steps for K-fold cross-validation\n",
    "Split the dataset into K equal partitions (or \"folds\")\n",
    "So if k = 5 and dataset has 150 observations\n",
    "Each of the 5 folds would have 30 observations\n",
    "Use fold 1 as the testing set and the union of the other folds as the training set\n",
    "Testing set = 30 observations (fold 1)\n",
    "Training set = 120 observations (folds 2-5)\n",
    "Calculate testing accuracy\n",
    "Repeat steps 2 and 3 K times, using a different fold as the testing set each time\n",
    "We will repeat the process 5 times\n",
    "2nd iteration\n",
    "fold 2 would be the testing set\n",
    "union of fold 1, 3, 4, and 5 would be the training set\n",
    "3rd iteration\n",
    "fold 3 would be the testing set\n",
    "union of fold 1, 2, 4, and 5 would be the training set\n",
    "And so on...\n",
    "Use the average testing accuracy as the estimate of out-of-sample accuracy\n",
    "Diagram of 5-fold cross-validation:\n",
    "\n",
    "5-fold cross-validation\n",
    "\n",
    "# simulate splitting a dataset of 25 observations into 5 folds\n",
    "from sklearn.cross_validation import KFold\n",
    "kf = KFold(25, n_folds=5, shuffle=False)\n",
    "\n",
    "\n",
    "# print the contents of each training and testing set\n",
    "# ^ - forces the field to be centered within the available space\n",
    "# .format() - formats the string similar to %s or %n\n",
    "# enumerate(sequence, start=0) - returns an enumerate object\n",
    "print('{} {:^61} {}'.format('Iteration', 'Training set obsevations', 'Testing set observations'))\n",
    "for iteration, data in enumerate(kf, start=1):\n",
    "    print('{!s:^9} {} {!s:^25}'.format(iteration, data[0], data[1]))\n",
    "Iteration                   Training set obsevations                    Testing set observations\n",
    "    1     [ 5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]        [0 1 2 3 4]\n",
    "    2     [ 0  1  2  3  4 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]        [5 6 7 8 9]\n",
    "    3     [ 0  1  2  3  4  5  6  7  8  9 15 16 17 18 19 20 21 22 23 24]     [10 11 12 13 14]\n",
    "    4     [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 20 21 22 23 24]     [15 16 17 18 19]\n",
    "    5     [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]     [20 21 22 23 24]\n",
    "Dataset contains 25 observations (numbered 0 through 24)\n",
    "5-fold cross-validation, thus it runs for 5 iterations\n",
    "For each iteration, every observation is either in the training set or the testing set, but not both\n",
    "Every observation is in the testing set exactly once\n",
    "3. Comparing cross-validation to train/test split\n",
    "Advantages of cross-validation:\n",
    "\n",
    "More accurate estimate of out-of-sample accuracy\n",
    "More \"efficient\" use of data\n",
    "This is because every observation is used for both training and testing\n",
    "Advantages of train/test split:\n",
    "\n",
    "Runs K times faster than K-fold cross-validation\n",
    "This is because K-fold cross-validation repeats the train/test split K-times\n",
    "Simpler to examine the detailed results of the testing process\n",
    "4. Cross-validation recommendations\n",
    "K can be any number, but K=10 is generally recommended\n",
    "This has been shown experimentally to produce the best out-of-sample estimate\n",
    "For classification problems, stratified sampling is recommended for creating the folds\n",
    "Each response class should be represented with equal proportions in each of the K folds\n",
    "If dataset has 2 response classes\n",
    "Spam/Ham\n",
    "20% observation = ham\n",
    "Each cross-validation fold should consist of exactly 20% ham\n",
    "scikit-learn's cross_val_score function does this by default\n",
    "5. Cross-validation example: parameter tuning\n",
    "Goal: Select the best tuning parameters (aka \"hyperparameters\") for KNN on the iris dataset\n",
    "\n",
    "We want to choose the best tuning parameters that best generalize the data\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "# 10-fold cross-validation with K=5 for KNN (the n_neighbors parameter)\n",
    "# k = 5 for KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Use cross_val_score function\n",
    "# We are passing the entirety of X and y, not X_train or y_train, it takes care of splitting the dat\n",
    "# cv=10 for 10 folds\n",
    "# scoring='accuracy' for evaluation metric - althought they are many\n",
    "scores = cross_val_score(knn, X, y, cv=10, scoring='accuracy')\n",
    "print(scores)\n",
    "[ 1.          0.93333333  1.          1.          0.86666667  0.93333333\n",
    "  0.93333333  1.          1.          1.        ]\n",
    "In the first iteration, the accuracy is 100%\n",
    "Second iteration, the accuracy is 93% and so on\n",
    "cross_val_score executes the first 4 steps of k-fold cross-validation steps which I have broken down to 7 steps here in detail\n",
    "\n",
    "Split the dataset (X and y) into K=10 equal partitions (or \"folds\")\n",
    "Train the KNN model on union of folds 2 to 10 (training set)\n",
    "Test the model on fold 1 (testing set) and calculate testing accuracy\n",
    "Train the KNN model on union of fold 1 and fold 3 to 10 (training set)\n",
    "Test the model on fold 2 (testing set) and calculate testing accuracy\n",
    "It will do this on 8 more times\n",
    "When finished, it will return the 10 testing accuracy scores as a numpy array\n",
    "# use average accuracy as an estimate of out-of-sample accuracy\n",
    "# numpy array has a method mean()\n",
    "print(scores.mean())\n",
    "0.966666666667\n",
    "Our goal here is to find the optimal value of K\n",
    "\n",
    "# search for an optimal value of K for KNN\n",
    "\n",
    "# range of k we want to try\n",
    "k_range = range(1, 31)\n",
    "# empty list to store scores\n",
    "k_scores = []\n",
    "\n",
    "# 1. we will loop through reasonable values of k\n",
    "for k in k_range:\n",
    "    # 2. run KNeighborsClassifier with k neighbours\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    # 3. obtain cross_val_score for KNeighborsClassifier with k neighbours\n",
    "    scores = cross_val_score(knn, X, y, cv=10, scoring='accuracy')\n",
    "    # 4. append mean of scores for k neighbors to k_scores list\n",
    "    k_scores.append(scores.mean())\n",
    "\n",
    "\n",
    "print(k_scores)\n",
    "[0.95999999999999996, 0.95333333333333337, 0.96666666666666656, 0.96666666666666656, 0.96666666666666679, 0.96666666666666679, 0.96666666666666679, 0.96666666666666679, 0.97333333333333338, 0.96666666666666679, 0.96666666666666679, 0.97333333333333338, 0.98000000000000009, 0.97333333333333338, 0.97333333333333338, 0.97333333333333338, 0.97333333333333338, 0.98000000000000009, 0.97333333333333338, 0.98000000000000009, 0.96666666666666656, 0.96666666666666656, 0.97333333333333338, 0.95999999999999996, 0.96666666666666656, 0.95999999999999996, 0.96666666666666656, 0.95333333333333337, 0.95333333333333337, 0.95333333333333337]\n",
    "# in essence, this is basically running the k-fold cross-validation method 30 times because we want to run through K values from 1 to 30\n",
    "# we should have 30 scores here\n",
    "print('Length of list', len(k_scores))\n",
    "print('Max of list', max(k_scores))\n",
    "Length of list 30\n",
    "Max of list 0.98\n",
    "# plot how accuracy changes as we vary k\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# plot the value of K for KNN (x-axis) versus the cross-validated accuracy (y-axis)\n",
    "# plt.plot(x_axis, y_axis)\n",
    "plt.plot(k_range, k_scores)\n",
    "plt.xlabel('Value of K for KNN')\n",
    "plt.ylabel('Cross-validated accuracy')\n",
    "<matplotlib.text.Text at 0x111cb07b8>\n",
    "\n",
    "The maximum cv accuracy occurs from k=13 to k=20\n",
    "\n",
    "The general shape of the curve is an upside down yield\n",
    "\n",
    "This is quite typical when examining the model complexity and accuracy\n",
    "This is an example of bias-variance trade off\n",
    "Low values of k (low bias, high variance)\n",
    "The 1-Nearest Neighbor classifier is the most complex nearest neighbor model\n",
    "It has the most jagged decision boundary, and is most likely to overfit\n",
    "High values of k (high bias, low variance)\n",
    "underfit\n",
    "Best value is the middle of k (most likely to generalize out-of-sample data)\n",
    "just right\n",
    "The best value of k\n",
    "\n",
    "Higher values of k produce less complex model\n",
    "So we will choose 20 as our best KNN model\n",
    "6. Cross-validation example: model selection\n",
    "Goal: Compare the best KNN model with logistic regression on the iris dataset\n",
    "\n",
    "# 10-fold cross-validation with the best KNN model\n",
    "knn = KNeighborsClassifier(n_neighbors=20)\n",
    "\n",
    "# Instead of saving 10 scores in object named score and calculating mean\n",
    "# We're just calculating the mean directly on the results\n",
    "print(cross_val_score(knn, X, y, cv=10, scoring='accuracy').mean())\n",
    "0.98\n",
    "# 10-fold cross-validation with logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "print(cross_val_score(logreg, X, y, cv=10, scoring='accuracy').mean())\n",
    "0.953333333333\n",
    "We can conclude that KNN is likely a better choice than logistic regression\n",
    "\n",
    "7. Cross-validation example: feature selection\n",
    "Goal: Select whether the Newspaper feature should be included in the linear regression model on the advertising dataset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# read in the advertising dataset\n",
    "data = pd.read_csv('http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv', index_col=0)\n",
    "# create a Python list of three feature names\n",
    "feature_cols = ['TV', 'Radio', 'Newspaper']\n",
    "\n",
    "# use the list to select a subset of the DataFrame (X)\n",
    "X = data[feature_cols]\n",
    "\n",
    "# select the Sales column as the response (y)\n",
    "# since we're selecting only one column, we can select the attribute using .attribute\n",
    "y = data.Sales\n",
    "# 10-fold cross-validation with all three features\n",
    "# instantiate model\n",
    "lm = LinearRegression()\n",
    "\n",
    "# store scores in scores object\n",
    "# we can't use accuracy as our evaluation metric since that's only relevant for classification problems\n",
    "# RMSE is not directly available so we will use MSE\n",
    "scores = cross_val_score(lm, X, y, cv=10, scoring='mean_squared_error')\n",
    "print(scores)\n",
    "[-3.56038438 -3.29767522 -2.08943356 -2.82474283 -1.3027754  -1.74163618\n",
    " -8.17338214 -2.11409746 -3.04273109 -2.45281793]\n",
    "MSE should be positive\n",
    "\n",
    "But why is the MSE here negative?\n",
    "MSE is a loss function\n",
    "It is something we want to minimize\n",
    "A design decision was made so that the results are made negative\n",
    "The best results would be the largest number (the least negative) so we can still maximize similar to classification accuracy\n",
    "Classification Accuracy is a reward function\n",
    "It is something we want to maximize\n",
    "# fix the sign of MSE scores\n",
    "mse_scores = -scores\n",
    "print(mse_scores)\n",
    "[ 3.56038438  3.29767522  2.08943356  2.82474283  1.3027754   1.74163618\n",
    "  8.17338214  2.11409746  3.04273109  2.45281793]\n",
    "# convert from MSE to RMSE\n",
    "rmse_scores = np.sqrt(mse_scores)\n",
    "print(rmse_scores)\n",
    "[ 1.88689808  1.81595022  1.44548731  1.68069713  1.14139187  1.31971064\n",
    "  2.85891276  1.45399362  1.7443426   1.56614748]\n",
    "# calculate the average RMSE\n",
    "print(rmse_scores.mean())\n",
    "1.69135317081\n",
    "# 10-fold cross-validation with two features (excluding Newspaper)\n",
    "feature_cols = ['TV', 'Radio']\n",
    "X = data[feature_cols]\n",
    "print(np.sqrt(-cross_val_score(lm, X, y, cv=10, scoring='mean_squared_error')).mean())\n",
    "1.67967484191\n",
    "Without Newspaper\n",
    "\n",
    "Average RMSE = 1.68\n",
    "lower number than with model with Newspaper\n",
    "RMSE is something we want to minimize\n",
    "So the model excluding Newspaper is a better model\n",
    "8. Improvements to cross-validation\n",
    "Repeated cross-validation\n",
    "\n",
    "Repeat cross-validation multiple times (with different random splits of the data) and average the results\n",
    "More reliable estimate of out-of-sample performance by reducing the variance associated with a single trial of cross-validation\n",
    "Creating a hold-out set\n",
    "\n",
    "\"Hold out\" a portion of the data before beginning the model building process\n",
    "Locate the best model using cross-validation on the remaining data, and test it using the hold-out set\n",
    "More reliable estimate of out-of-sample performance since hold-out set is truly out-of-sample\n",
    "Feature engineering and selection within cross-validation iterations\n",
    "\n",
    "Normally, feature engineering and selection occurs before cross-validation\n",
    "Instead, perform all feature engineering and selection within each cross-validation iteration\n",
    "More reliable estimate of out-of-sample performance since it better mimics the application of the model to out-of-sample data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
